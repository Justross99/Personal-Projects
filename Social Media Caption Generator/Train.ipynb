{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media Caption Generator\n",
    "\n",
    "##### Objective:\n",
    "- Generate a suggested caption for a social media post.\n",
    "- take an image as input and use a transformer decoder to generate text.\n",
    "- The point of this model is to explore other use cases for transformer models with nonstandard inputs\n",
    "\n",
    "##### Inspired by:\n",
    "- AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\n",
    "- Perceiver: General Perception with Iterative Attention\n",
    "\n",
    "- Originally, model was planned as a visual transformer encoder (ViT) into a language transformer decoder\n",
    "- In place of encoder, I implement the Perceiver architecture transformer and feed its outputs into a language transformer decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # \"CPU\"\n",
    "\n",
    "# this is to check if the gpu is present\n",
    "print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Embedding, LayerNormalization, Input\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow_addons.activations import gelu\n",
    "from tensorflow_addons.optimizers import LAMB, RectifiedAdam\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import imageio\n",
    "from cv2 import resize\n",
    "from imgaug import augmenters as iaa\n",
    "from glob import glob\n",
    "\n",
    "from math import pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select appropriate policy for hardware\n",
    "policy = tf.keras.mixed_precision.Policy(\"mixed_float16\")#float32, float16\n",
    "tf.keras.mixed_precision.set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data is preprocessed and stored on disk within the data preprocessing notebook\n",
    "- The importance of the data preprocessing notebook to be seperate comes from memory limit issues, particularly VRAM\n",
    "\n",
    "Structure of the data is as follows:\n",
    "- InceptionV3 image features for each post within image_encodings folder\n",
    "- Captions indexed by vocabulary defined in preprocessing within tokenized_captions folder\n",
    "- both files related to a post share the same name (without extension) and filename lists are used to split and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./other/vocab.pkl\", 'rb')\n",
    "vocab = pickle.load(f)\n",
    "f.close()\n",
    "vocab_inverse = {idx: w for w, idx in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = 1\n",
    "unk_idx = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = \"./images/\"\n",
    "filename_list = []\n",
    "for img in tqdm(glob(IMAGE_PATH + \"*.jpg\")):\n",
    "    filename_list.append(os.path.basename(img))\n",
    "\n",
    "train_filenames = filename_list[:-10000]\n",
    "val_filenames = filename_list[-10000:]\n",
    "del filename_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_filenames))\n",
    "print(len(val_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "N_EPOCHS = 12\n",
    "MAX_LEN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 128\n",
    "N_CHANNELS = 3\n",
    "SEQUENCE_LENGTH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FREQ_BANDS = 16\n",
    "MAX_FREQ = 64\n",
    "FREQ_BASE = 2\n",
    "INPUT_AXIS = 2\n",
    "DATA_CHANNELS = N_CHANNELS + (INPUT_AXIS * ((NUM_FREQ_BANDS * 2) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.zeros((len(train_filenames), IMG_SIZE, IMG_SIZE, 3), dtype=np.float16)\n",
    "train_captions = []\n",
    "for i, filename in tqdm(enumerate(train_filenames)):\n",
    "    train_images[i] = resize(imageio.imread(\"./images/\" + filename[:-4] + \".jpg\"), (IMG_SIZE, IMG_SIZE))/255\n",
    "    train_captions.append(np.load(\"./tokenized_caption/\" + filename[:-4] + \".npy\"))\n",
    "\n",
    "val_images = np.zeros((len(val_filenames), IMG_SIZE, IMG_SIZE, 3), dtype=np.float16)\n",
    "val_captions = []\n",
    "for i, filename in tqdm(enumerate(val_filenames)):\n",
    "    val_images[i] = resize(imageio.imread(\"./images/\" + filename[:-4] + \".jpg\"), (IMG_SIZE, IMG_SIZE))/255\n",
    "    val_captions.append(np.load(\"./tokenized_caption/\" + filename[:-4] + \".npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_aug = iaa.RandAugment(n=3, m=7)\n",
    "\n",
    "#Credit for fourier_encode function from lucidrains/perceiver\n",
    "def fourier_encode(x, max_freq, num_bands, base):\n",
    "    x = tf.expand_dims(x, -1)\n",
    "    x_orig = x\n",
    "    scales = tf.experimental.numpy.logspace(start=0,\n",
    "                                            stop=tf.math.log(tf.cast(max_freq/2, x.dtype))/tf.math.log(tf.cast(base, x.dtype)),\n",
    "                                            num=num_bands,\n",
    "                                            endpoint=True,\n",
    "                                            base=base,\n",
    "                                            dtype=x.dtype)\n",
    "    scales = scales[(*((None,) * (len(x.shape) - 1)), Ellipsis)]\n",
    "    \n",
    "    x = x * scales * pi\n",
    "    x = tf.concat([tf.math.sin(x), tf.math.cos(x)], axis=-1)\n",
    "    x = tf.concat((x, x_orig), axis=-1)\n",
    "    return x\n",
    "    \n",
    "def make_fourier_grid(num_freq_bands, max_freq, freq_base, dims):\n",
    "        axis_pos = tf.map_fn(fn = lambda size: tf.linspace(start=-1, stop=1, num=size),\n",
    "                             elems=tf.convert_to_tensor(dims, dtype=tf.int32), fn_output_signature=tf.float64)\n",
    "        \n",
    "        x_axis = axis_pos[0,:]\n",
    "        y_axis = axis_pos[1,:]\n",
    "\n",
    "        pos = tf.cast(tf.stack(tf.meshgrid(x_axis, y_axis), axis=-1), policy.variable_dtype)\n",
    "        enc_pos = fourier_encode(pos, max_freq, num_freq_bands, freq_base)\n",
    "        enc_pos_shape = tf.shape(enc_pos)\n",
    "        enc_pos = tf.reshape(enc_pos, tf.concat([enc_pos_shape[0:2],enc_pos_shape[2,tf.newaxis]*enc_pos_shape[3]], axis=0))        \n",
    "        return enc_pos\n",
    "\n",
    "def batch_captions_to_matrix(batch_captions, max_len=50):\n",
    "        # function takes a list of tokenized captions, pads and returns a matrix for the batch\n",
    "        batch = np.full((len(batch_captions), max_len), pad_idx)\n",
    "        for i, caption in enumerate(batch_captions):\n",
    "            batch[i,:len(caption)] = caption[:max_len]\n",
    "        return batch\n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    # Generator uses list of filenames to load in the data\n",
    "    def __init__(self, img_tensor, caption_list, batch_size=64, sequence_length=50, shuffle=True, image_size=300, n_channels=3,\n",
    "                 num_frequency_bands=64, max_freq=200, freq_base=2, input_axis=2, augment=True):\n",
    "        self.image_size = image_size\n",
    "        self.n_channels = n_channels\n",
    "        self.img_tensor = img_tensor\n",
    "        self.caption_list = caption_list\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        self.n = 0\n",
    "        self.n_total = self.__len__()\n",
    "        self.augment = augment\n",
    "        self.fourier_grid = make_fourier_grid(num_frequency_bands, max_freq, freq_base, [image_size, image_size])\n",
    "        self.data_channels = n_channels + (input_axis * ((num_frequency_bands * 2) + 1))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.caption_list)/self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indices = self.indices[index*self.batch_size : (index + 1)*self.batch_size]\n",
    "        \n",
    "        batch_images = np.zeros((self.batch_size, self.image_size, self.image_size, self.n_channels))\n",
    "        captions_tokenized = []\n",
    "        for i, ind in enumerate(indices):\n",
    "            batch_images[i] = self.img_tensor[ind]\n",
    "            captions_tokenized.append(self.caption_list[ind])\n",
    "\n",
    "        batch_captions = batch_captions_to_matrix(captions_tokenized)\n",
    "\n",
    "        batch_images = batch_images*255\n",
    "        if self.augment:\n",
    "            batch_images = rand_aug(images=batch_images.astype(np.uint8))\n",
    "        batch_images = (batch_images.astype(np.float64) / (255/2)) - 1\n",
    "        \n",
    "        enc_pos = tf.tile(self.fourier_grid, [self.batch_size, 1, 1])\n",
    "        enc_pos = tf.reshape(enc_pos, tf.concat([[self.batch_size], tf.shape(self.fourier_grid)], axis=0))        \n",
    "        data = tf.concat((batch_images, enc_pos), axis=-1)\n",
    "\n",
    "        # concatenate and flatten\n",
    "        # change data from batch, height, width, channel to batch, height/width, channel\n",
    "        data_shape = tf.shape(data)\n",
    "        data = tf.reshape(data, tf.concat([data_shape[0, tf.newaxis], data_shape[1,tf.newaxis]*data_shape[2], data_shape[3,tf.newaxis]], axis=0))\n",
    "        \n",
    "        return tf.cast(data, policy.variable_dtype), tf.cast(batch_captions, tf.int32)\n",
    "\n",
    "\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.n >= self.n_total:\n",
    "            self.n = 0\n",
    "            if self.shuffle:\n",
    "                self.on_epoch_end()\n",
    "        result = self.__getitem__(self.n)\n",
    "        self.n += 1\n",
    "        return result\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        # shuffles\n",
    "        self.indices = np.arange(len(self.caption_list))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_generator = DataGenerator(\n",
    "            train_images,\n",
    "            train_captions,\n",
    "            batch_size = BATCH_SIZE,\n",
    "            sequence_length=50,\n",
    "            image_size=IMG_SIZE,\n",
    "            num_frequency_bands=NUM_FREQ_BANDS,\n",
    "            max_freq=MAX_FREQ,\n",
    "            freq_base=FREQ_BASE)\n",
    "\n",
    "validation_data_generator = DataGenerator(\n",
    "            val_images,\n",
    "            val_captions,\n",
    "            batch_size = BATCH_SIZE,\n",
    "            sequence_length=50,\n",
    "            image_size=IMG_SIZE,\n",
    "            num_frequency_bands=NUM_FREQ_BANDS,\n",
    "            max_freq=MAX_FREQ,\n",
    "            freq_base=FREQ_BASE,\n",
    "            augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "ex_img_sequences, ex_cap = train_data_generator.__getitem__(1)\n",
    "e = time.time()\n",
    "print(\"elapsed_time: \" + str(e-t))\n",
    "print(\"ex_img_encoding\")\n",
    "print(ex_img_sequences.shape)\n",
    "print(ex_img_sequences)\n",
    "\n",
    "del ex_img_sequences, ex_cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:CPU:0'):\n",
    "    train_dataset = tf.data.Dataset.from_generator(lambda: train_data_generator,\n",
    "                                       output_signature=(\n",
    "                                           tf.TensorSpec(shape=(BATCH_SIZE, IMG_SIZE * IMG_SIZE, DATA_CHANNELS), dtype=policy.variable_dtype),\n",
    "                                           tf.TensorSpec(shape=(BATCH_SIZE, SEQUENCE_LENGTH), dtype=tf.int32)))\n",
    "    train_dataset = train_dataset.prefetch(8)\n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_generator(lambda: validation_data_generator,\n",
    "                                       output_signature=(\n",
    "                                           tf.TensorSpec(shape=(BATCH_SIZE, IMG_SIZE * IMG_SIZE, DATA_CHANNELS), dtype=policy.variable_dtype),\n",
    "                                           tf.TensorSpec(shape=(BATCH_SIZE, SEQUENCE_LENGTH), dtype=tf.int32)))\n",
    "    val_dataset = val_dataset.prefetch(8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the observed test above, retrieving the data from memory this way should not cause a significant slowdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(jit_compile=True)\n",
    "def attention(query, key, value, mask=None):        \n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], score.dtype)\n",
    "        scaled_score = tf.math.divide(score, tf.math.sqrt(dim_key))\n",
    "        if mask is not None:\n",
    "            scaled_score += (mask * (scaled_score.dtype.min/32)) #to avoid numerical underflow/overflow\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1) # (..., seq_len_q, seq_len_k)\n",
    "        output = tf.matmul(weights, value) # (..., seq_len_q, depth_v)\n",
    "        return output, weights\n",
    "\n",
    "class MultiHeadSelfAttention(Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8, has_bias=True):#, is_causal=False):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\")\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "\n",
    "        self.query_dense = Dense(embed_dim)\n",
    "        self.key_dense = Dense(embed_dim)\n",
    "        self.value_dense = Dense(embed_dim)\n",
    "\n",
    "        self.combine_heads = Dense(embed_dim)\n",
    "\n",
    "    @tf.function(jit_compile=True)\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x, padding_mask=None):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        \n",
    "        query = self.query_dense(x)  # (batch_size, seq_len, embed_dim)\n",
    "        key =   self.key_dense(x)    # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(x)  # (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        query = self.separate_heads(query, batch_size)  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key =   self.separate_heads(key, batch_size)  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(value, batch_size)  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        \n",
    "        attention_out, _ = attention(query, key, value, padding_mask)\n",
    "        \n",
    "        attention_out = tf.transpose(attention_out, perm=[0, 2, 1, 3])  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(attention_out, (batch_size, -1, self.embed_dim))  # (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        output = self.combine_heads(concat_attention)  # (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        return output\n",
    "        \n",
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8, has_bias=True):#, is_causal=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\")\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "\n",
    "        self.query_dense = Dense(embed_dim)\n",
    "        self.key_dense = Dense(embed_dim)\n",
    "        self.value_dense = Dense(embed_dim)\n",
    "        \n",
    "        self.combine_heads = Dense(embed_dim)\n",
    "\n",
    "    @tf.function(jit_compile=True)\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, q, kv, padding_mask=None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        query = self.query_dense(q)  # (batch_size, seq_len, embed_dim)\n",
    "        key =   self.key_dense(kv)    # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(kv)  # (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        query = self.separate_heads(query, batch_size)  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key =   self.separate_heads(key, batch_size)  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(value, batch_size)  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        \n",
    "        attention_out, weights = attention(query, key, value, padding_mask)\n",
    "        attention_out = tf.transpose(attention_out, perm=[0, 2, 1, 3])  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(attention_out, (batch_size, -1, self.embed_dim))  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(concat_attention)  # (batch_size, seq_len, embed_dim)\n",
    "        return output, weights\n",
    "\n",
    "class PerceiverTransformerBlock(Layer):\n",
    "    def __init__(self,\n",
    "                 embed_dim,\n",
    "                 num_latents = 512,\n",
    "                 cross_heads = 8,\n",
    "                 latent_heads = 8,\n",
    "                 transformers_per_attend = 1):\n",
    "        super(PerceiverTransformerBlock, self).__init__()\n",
    "        \n",
    "        #cross attention block\n",
    "        self.cross_attn_layernorm_x = LayerNormalization(epsilon=1e-3)\n",
    "        self.cross_attn = MultiHeadAttention(embed_dim, cross_heads)\n",
    "        self.cross_ffn = Sequential([LayerNormalization(epsilon=1e-3), Dense(embed_dim, activation=gelu), Dense(embed_dim)])\n",
    "\n",
    "        #self attention block\n",
    "        self.transformers_per_attend = transformers_per_attend\n",
    "        self.self_attention_transformers = [Sequential([LayerNormalization(epsilon=1e-3),\n",
    "                                                        MultiHeadSelfAttention(embed_dim, latent_heads)])\n",
    "                                                        for i in range(transformers_per_attend)]\n",
    "        self.self_attention_transformer_ffns = [Sequential([LayerNormalization(epsilon=1e-3),\n",
    "                                                            Dense(embed_dim, activation=gelu),\n",
    "                                                            Dense(embed_dim)]) for i in range(transformers_per_attend)]\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x, data):\n",
    "        shortcut1, _ = self.cross_attn(self.cross_attn_layernorm_x(x), data) #input to attn is (q,k,v,mask)\n",
    "        x = x + shortcut1\n",
    "        shortcut2 = self.cross_ffn(x)\n",
    "        x = x + shortcut2\n",
    "        for i in range(self.transformers_per_attend):\n",
    "            shortcut1 = self.self_attention_transformers[i](x)\n",
    "            x = x + shortcut1\n",
    "            \n",
    "            shortcut2 = self.self_attention_transformer_ffns[i](x)\n",
    "            x = x + shortcut2\n",
    "        return x\n",
    "\n",
    "class Perceiver(Layer):\n",
    "    def __init__(self, num_layers, latent_dim, num_latents=512, cross_heads=8,\n",
    "                 latent_heads=8, weight_tie=True, self_attn_transformers_per_attend=1):\n",
    "        super(Perceiver, self).__init__()\n",
    "        self.context_norm = LayerNormalization(epsilon=1e-3)\n",
    "        self.num_layers = num_layers\n",
    "        self.first_layer = PerceiverTransformerBlock(latent_dim,\n",
    "                                                     num_latents,\n",
    "                                                     cross_heads,\n",
    "                                                     latent_heads,\n",
    "                                                     self_attn_transformers_per_attend)\n",
    "        self.weight_tie_layers = weight_tie\n",
    "        if weight_tie is False:\n",
    "            self.perceiver_layers = [PerceiverTransformerBlock(latent_dim,\n",
    "                                                               num_latents,\n",
    "                                                               cross_heads,\n",
    "                                                               latent_heads,\n",
    "                                                               self_attn_transformers_per_attend) for i in range(num_layers-1)]\n",
    "        else:\n",
    "            self.perceiver_layers = PerceiverTransformerBlock(latent_dim,\n",
    "                                                              num_latents,\n",
    "                                                              cross_heads,\n",
    "                                                              latent_heads,\n",
    "                                                              self_attn_transformers_per_attend)\n",
    "\n",
    "        latents = tf.clip_by_value(tf.random.normal((num_latents, latent_dim), mean=0, stddev=0.02), -2, 2)\n",
    "        self.latents = tf.Variable(initial_value=latents, trainable=True)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, data):\n",
    "        batch_size = tf.shape(data)[0]\n",
    "        \n",
    "        x = tf.tile(self.latents, [batch_size, 1])\n",
    "        x = tf.reshape(x, tf.concat([[batch_size], tf.shape(self.latents)], axis=0))\n",
    "        data = self.context_norm(data)\n",
    "\n",
    "        x = self.first_layer(x, data)\n",
    "        if self.weight_tie_layers is True:\n",
    "            for i in range(self.num_layers - 1):\n",
    "                x = self.perceiver_layers(x, data)\n",
    "        else:\n",
    "            for i in range(self.num_layers - 1):\n",
    "                x = self.perceiver_layers[i](x, data)\n",
    "        return x\n",
    "\n",
    "class DecoderTransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(DecoderTransformerBlock, self).__init__()\n",
    "        \n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)#1e-6\n",
    "        self.att1 = MultiHeadSelfAttention(embed_dim, num_heads) #(batch_size, seq_len, embed_dim)\n",
    "        self.ffn1 = Sequential([LayerNormalization(epsilon=1e-6), Dense(ff_dim, activation=gelu), Dense(embed_dim)]) #(batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)#1e-6\n",
    "        self.att2 = MultiHeadAttention(embed_dim, num_heads) #(batch_size, seq_len, embed_dim)\n",
    "        self.ffn2 = Sequential([LayerNormalization(epsilon=1e-6), Dense(ff_dim, activation=gelu), Dense(embed_dim)])\n",
    "\n",
    "    @tf.function(jit_compile=True)\n",
    "    def call(self, inputs, ENCODER_OUTPUT, training, look_ahead_mask, padding_mask):\n",
    "        x = self.layernorm1(inputs)\n",
    "        attn1_output = self.att1(x, look_ahead_mask) #(batch_size, input_seq_len, embed_dim)\n",
    "        x = x + attn1_output\n",
    "        shortcut = self.ffn1(x)\n",
    "        x = x + shortcut\n",
    "        attn2_output, _ = self.att2(x, ENCODER_OUTPUT, None)\n",
    "        x = x + attn2_output\n",
    "        shortcut = self.ffn2(x)\n",
    "        x = x + shortcut\n",
    "        return x\n",
    "\n",
    "class Decoder(Layer):\n",
    "    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, sequence_length, target_vocab_size, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.sequence_length = sequence_length -1\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = Embedding(input_dim=target_vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = Embedding(input_dim=sequence_length, output_dim=embed_dim)\n",
    "        self.dec_layers = [DecoderTransformerBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(num_layers)]\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        positions = tf.range(start=0, limit=self.sequence_length, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x += positions\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "        return x  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, n_perceiver_layers,\n",
    "                 n_dec_layers, embed_dim, dec_heads, dec_ff_dim, sequence_length, target_vocab_size,\n",
    "                 num_latents=512, cross_heads=8, weight_tie_perceiver_layers=True,\n",
    "                 latent_heads=8, dropout_rate=0.1, self_attn_transformers_per_attend=1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.encoder = Perceiver(n_perceiver_layers,\n",
    "                                 embed_dim,\n",
    "                                 num_latents,\n",
    "                                 cross_heads,\n",
    "                                 latent_heads,\n",
    "                                 weight_tie_perceiver_layers,\n",
    "                                 self_attn_transformers_per_attend)\n",
    "        \n",
    "        self.decoder = Decoder(n_dec_layers,\n",
    "                               embed_dim,\n",
    "                               dec_heads,\n",
    "                               dec_ff_dim,\n",
    "                               sequence_length,\n",
    "                               target_vocab_size,\n",
    "                               dropout_rate)\n",
    "        self.final_layer = Dense(target_vocab_size, dtype=\"float32\")\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, image_sequence, target, training, look_ahead_mask, padding_mask):\n",
    "        enc_output = self.encoder(image_sequence)  # (batch_size, inp_seq_len*d_model)\n",
    "        decoder_output = self.decoder(target, enc_output, training, look_ahead_mask, padding_mask) # (batch_size, inp_seq_len, d_model)\n",
    "        final_output = self.final_layer(decoder_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_params = {\n",
    "    \"n_perceiver_layers\": 2,\n",
    "    \"n_dec_layers\": 2,\n",
    "    \"embed_dim\": 512,\n",
    "    \"dec_heads\": 8,\n",
    "    \"dec_ff_dim\": 512,\n",
    "    \"sequence_length\": 50,\n",
    "    \"target_vocab_size\": len(vocab),\n",
    "    \"num_latents\": 512,\n",
    "    \"cross_heads\": 1,\n",
    "    \"weight_tie_perceiver_layers\":True,\n",
    "    \"latent_heads\": 4,\n",
    "    \"dropout_rate\": 0,\n",
    "    \"self_attn_transformers_per_attend\": 2\n",
    "    }\n",
    "Caption_generator = Transformer(**transformer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to build model before going into tf.function\n",
    "temp1, temp2 = validation_data_generator.__getitem__(0)\n",
    "_ = Caption_generator(temp1, temp2[:, :-1], False, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Caption_generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(jit_compile=True)\n",
    "def create_padding_mask(batch):\n",
    "    mask = tf.cast(tf.math.equal(batch, 0), policy.compute_dtype)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "@tf.function(jit_compile=True)\n",
    "def create_look_ahead_mask(batch):\n",
    "    batch_size, seq_len = batch.shape\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    return tf.cast(mask, policy.compute_dtype)\n",
    "\n",
    "@tf.function(jit_compile=True)\n",
    "def create_masks(batch):\n",
    "    padding_mask = create_padding_mask(batch)\n",
    "    look_ahead_mask = create_look_ahead_mask(batch)\n",
    "    combined_mask = tf.maximum(padding_mask, look_ahead_mask)\n",
    "    return combined_mask, padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning_rate = 0.004 #DIVERGES\n",
    "learning_rate = 0.0004 #GOOD\n",
    "#learning_rate = 0.00004 #NAN (underflow)\n",
    "optim = LAMB(learning_rate)\n",
    "#optimizer = RectifiedAdam(learning_rate=1e-4, total_steps=10000, warmup_proportion=0.1, min_lr=1e-6)\n",
    "optim = tf.keras.mixed_precision.LossScaleOptimizer(optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=\"none\")\n",
    "@tf.function\n",
    "def loss_function(batch_captions, predictions):\n",
    "    mask = tf.math.logical_not(tf.math.equal(batch_captions, pad_idx))\n",
    "    loss_ = loss_obj(batch_captions, predictions)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "@tf.function(jit_compile=True)\n",
    "def accuracy_function(batch_captions, predictions):\n",
    "    accuracies = tf.equal(tf.cast(batch_captions, tf.int32), tf.cast(tf.argmax(predictions, axis=2), tf.int32))\n",
    "    mask = tf.math.logical_not(tf.math.equal(batch_captions, pad_idx))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/Perceiver/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=Caption_generator,\n",
    "                           optimizer=optim)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "train_accuracy = tf.keras.metrics.Mean(name=\"train_accuracy\")\n",
    "validation_loss = tf.keras.metrics.Mean(name=\"validation_loss\")\n",
    "validation_accuracy = tf.keras.metrics.Mean(name=\"validation_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(image_encodings_sequence, batch_captions):\n",
    "    batch_captions_input = batch_captions[:, :-1]\n",
    "    batch_captions_real = batch_captions[:, 1:]\n",
    "    \n",
    "    combined_mask, padding_mask = create_masks(batch_captions_input)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = Caption_generator(image_encodings_sequence, batch_captions_input,\n",
    "                                   True, combined_mask, padding_mask)\n",
    "        loss = loss_function(batch_captions_real, predictions)\n",
    "        scaled_loss = optim.get_scaled_loss(loss)\n",
    "    \n",
    "    if not tf.math.is_nan(scaled_loss):\n",
    "        scaled_gradients = tape.gradient(scaled_loss, Caption_generator.trainable_variables)\n",
    "        gradients = optim.get_unscaled_gradients(scaled_gradients)\n",
    "        optim.apply_gradients(zip(gradients, Caption_generator.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy_function(batch_captions_real, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def validation_step(image_encodings_sequence, batch_captions):\n",
    "    batch_captions_input = batch_captions[:, :-1]\n",
    "    batch_captions_real = batch_captions[:, 1:]\n",
    "    \n",
    "    combined_mask, padding_mask = create_masks(batch_captions_input)\n",
    "\n",
    "    predictions = Caption_generator(image_encodings_sequence, batch_captions_input,\n",
    "                               True, combined_mask, padding_mask)\n",
    "    loss = loss_function(batch_captions_real, predictions)\n",
    "    \n",
    "    validation_loss(loss)\n",
    "    validation_accuracy(accuracy_function(batch_captions_real, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to build model before going into tf.function\n",
    "temp1, temp2 = validation_data_generator.__getitem__(0)\n",
    "lha, pm = create_masks(temp2[:, :-1])\n",
    "_ = Caption_generator(temp1, temp2[:, :-1], False, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    train_iterator = iter(train_dataset)\n",
    "    validation_iterator = iter(val_dataset)\n",
    "    best_val_loss = 10\n",
    "    for epoch in range(epochs):\n",
    "        for batch_number in range(len(train_data_generator)):\n",
    "            image_encoding_sequence, captions = next(train_iterator)\n",
    "            train_step(image_encoding_sequence, captions)\n",
    "            \n",
    "            if batch_number % 100 == 0:\n",
    "                val_imgs, val_captions = next(validation_iterator)\n",
    "                validation_step(val_imgs, val_captions)\n",
    "                \n",
    "                print(f\"Epoch {epoch} Batch [{batch_number}/{len(train_data_generator)}]: train Loss: {train_loss.result():.5f}, train accuracy: {train_accuracy.result():.5f}, val Loss: {validation_loss.result():.5f}, val accuracy: {validation_accuracy.result():.5f}\")\n",
    "                \n",
    "                if validation_loss.result() < best_val_loss:\n",
    "                    best_val_loss = validation_loss.result()\n",
    "                    ckpt_manager.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LAMB lr=0.0004\n",
    "train(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr changed from 0.0004 to 0.00004\n",
    "train(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting model into encoder and decoder\n",
    "- I split the model into 2 parts so that images do not reprocess each time a new word is generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same as defined as above but inherits from tf.keras.Model instead of layer\n",
    "\n",
    "class Perceiver_encoder(tf.keras.Model):\n",
    "    def __init__(self, num_layers, latent_dim, num_latents=512, cross_heads=8,\n",
    "                 latent_heads=8, weight_tie=True, self_attn_transformers_per_attend=1):\n",
    "        super(Perceiver_encoder, self).__init__()\n",
    "        self.encoder = Perceiver(num_layers, latent_dim, num_latents, cross_heads,\n",
    "                                 latent_heads, weight_tie, self_attn_transformers_per_attend)\n",
    "    @tf.function(experimental_follow_type_hints=True)\n",
    "    def call(self, data):\n",
    "        x = self.encoder(data)\n",
    "        return x\n",
    "\n",
    "#initialize_model\n",
    "Standalone_Perceiver_encoder = Perceiver_encoder(num_layers=transformer_params[\"n_perceiver_layers\"],\n",
    "                              latent_dim=transformer_params[\"embed_dim\"],\n",
    "                              num_latents=transformer_params[\"num_latents\"],\n",
    "                              cross_heads=transformer_params[\"cross_heads\"],\n",
    "                              latent_heads=transformer_params[\"latent_heads\"],\n",
    "                              weight_tie=True,\n",
    "                              self_attn_transformers_per_attend=transformer_params[\"self_attn_transformers_per_attend\"])\n",
    "\n",
    "#to build model before going into tf.function\n",
    "temp1, temp2 = validation_data_generator.__getitem__(0)\n",
    "temp1 = Standalone_Perceiver_encoder(temp1)\n",
    "#copy_weights\n",
    "Standalone_Perceiver_encoder.set_weights(Caption_generator.layers[0].get_weights())\n",
    "Standalone_Perceiver_encoder.latents = Caption_generator.encoder.latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Standalone_Perceiver_encoder.get_weights()[0].shape)\n",
    "print(Caption_generator.layers[0].get_weights()[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Standalone_Perceiver_encoder.save(\"final_model/Perceiver_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(Standalone_Perceiver_encoder, \"final_model/saved_model_rest/Perceiver_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define decoder structure\n",
    "class Caption_generator_decoder(tf.keras.Model):\n",
    "    def __init__(self, n_dec_layers, embed_dim, dec_heads, dec_ff_dim, sequence_length, target_vocab_size, rate=0.1):\n",
    "        super(Caption_generator_decoder, self).__init__()\n",
    "        self.decoder = Decoder(n_dec_layers,\n",
    "                               embed_dim,\n",
    "                               dec_heads,\n",
    "                               dec_ff_dim,\n",
    "                               sequence_length,\n",
    "                               target_vocab_size,\n",
    "                               rate)\n",
    "        self.final_layer = Dense(target_vocab_size, dtype=\"float32\")\n",
    "    \n",
    "    @tf.function(experimental_follow_type_hints=True)\n",
    "    def call(self, inputs):\n",
    "        enc_output, target, training, look_ahead_mask, padding_mask = inputs\n",
    "        decoder_output = self.decoder(target, enc_output, training, look_ahead_mask, padding_mask) # (batch_size, inp_seq_len, d_model)\n",
    "        final_output = self.final_layer(decoder_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "        return final_output\n",
    "        \n",
    "#initialize_model\n",
    "Standalone_Transformer_decoder = Caption_generator_decoder(n_dec_layers=transformer_params[\"n_dec_layers\"],\n",
    "                                                embed_dim=transformer_params[\"embed_dim\"],\n",
    "                                                dec_heads=transformer_params[\"dec_heads\"],\n",
    "                                                dec_ff_dim=transformer_params[\"dec_ff_dim\"],\n",
    "                                                sequence_length=transformer_params[\"sequence_length\"],\n",
    "                                                target_vocab_size=transformer_params[\"target_vocab_size\"],\n",
    "                                                rate=transformer_params[\"dropout_rate\"])\n",
    "\n",
    "\n",
    "#build computation graph\n",
    "lha, pm = create_masks(temp2[:,:-1])\n",
    "_ = Standalone_Transformer_decoder([temp1, temp2[:, :-1], False, lha, pm])\n",
    "#copy_weights\n",
    "Standalone_Transformer_decoder.layers[0].set_weights(Caption_generator.layers[1].get_weights())\n",
    "Standalone_Transformer_decoder.layers[1].set_weights(Caption_generator.layers[2].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Standalone_Transformer_decoder.save(\"final_model/Transformer_decoder\", signatures=Standalone_Transformer_decoder.call.get_concrete_function(\n",
    "    [\n",
    "        tf.TensorSpec(shape=[None, 512, 512],  dtype=tf.float32, name=\"enc_output\"),\n",
    "        tf.TensorSpec(shape=[None, 49],        dtype=tf.int32,   name=\"target\"),\n",
    "        tf.TensorSpec(shape=[None],            dtype=tf.bool,    name=\"training\"),\n",
    "        tf.TensorSpec(shape=[None, 1, 49, 49], dtype=tf.float16, name=\"look_ahead_mask\"),\n",
    "        tf.TensorSpec(shape=[None, 1, 49, 49], dtype=tf.float16, name=\"padding_mask\")\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caption Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = \"#START#\"\n",
    "END = \"#END#\"\n",
    "PAD = \"#PAD#\"\n",
    "UNK = \"#UNK#\"\n",
    "PUNCTUATION_EXCLAMATION = \"PUNCTUATION_EXCLAMATION\"\n",
    "PUNCTUATION_PERIOD = \"PUNCTUATION_PERIOD\"\n",
    "PUNCTUATION_QUESTION_MARK = \"PUNCTUATION_QUESTION_MARK\"\n",
    "EXCLAMATION = \"EXCLAMATION\"\n",
    "def indices_to_caption(caption):\n",
    "    # Takes predicted tokens and returns a string.\n",
    "    # Removes words which are repeated too many times\n",
    "    predicted_caption = \"\"\n",
    "    prev_word = -1\n",
    "    same_word_count = 0\n",
    "    for word_ind in caption:\n",
    "        if word_ind != prev_word:\n",
    "            prev_word = word_ind\n",
    "            same_word_count = 0\n",
    "        if word_ind == prev_word:\n",
    "            same_word_count += 1\n",
    "        if same_word_count > 2:\n",
    "            continue\n",
    "        if word_ind == vocab[START]:\n",
    "            continue\n",
    "        if word_ind == vocab[END]:\n",
    "            return predicted_caption\n",
    "        if word_ind == vocab[PAD]:\n",
    "            continue\n",
    "        if word_ind == vocab[UNK]:\n",
    "            predicted_caption += \"___ \"\n",
    "            continue\n",
    "        if word_ind == vocab[EXCLAMATION]:\n",
    "            predicted_caption += \"! \"\n",
    "            continue\n",
    "        predicted_caption += vocab_inverse[word_ind] + \" \"\n",
    "    return predicted_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(encoded_image):\n",
    "    '''Generates a caption for input images. Images can either be batched with size BATCH_SIZE or can be a single image\n",
    "       NOTE: single images must be in the shape [1, encoded_image_size]'''\n",
    "    is_batch = True\n",
    "    n_images= encoded_image.shape\n",
    "    if n_images == 1:\n",
    "        is_batch = False\n",
    "        encoded_image = np.stack([encoded_image for _ in range(BATCH_SIZE)])\n",
    "\n",
    "    empty_cap = np.full((BATCH_SIZE, MAX_LEN), pad_idx)\n",
    "    empty_cap[:,0] = vocab[START]\n",
    "    empty_cap = empty_cap[:,:-1]\n",
    "\n",
    "    encoder_out = Standalone_Perceiver_encoder(encoded_image)\n",
    "    for i in range(48):\n",
    "        lha, pm = create_masks(empty_cap)\n",
    "\n",
    "        preds = Standalone_Transformer_decoder([encoder_out, empty_cap, False, lha, pm]).numpy()\n",
    "        # UNK tokens are masked out for caption generation\n",
    "        preds[:,:,3] = -3e4\n",
    "        \n",
    "        values, indices = tf.math.top_k(preds[:,i], 5)\n",
    "        values = tf.nn.softmax(values)\n",
    "        chosen = tf.random.categorical(values, 1).numpy()\n",
    "        empty_cap[:,i+1] = np.choose(chosen.T[0], indices.T)\n",
    "\n",
    "    predicted_caption = \"\"\n",
    "    predicted_captions = []\n",
    "    for caption in empty_cap:\n",
    "        if not is_batch:\n",
    "            predicted_caption = indices_to_caption(caption)\n",
    "            return predicted_caption\n",
    "        else:\n",
    "            predicted_caption = indices_to_caption(caption)\n",
    "            predicted_captions.append(predicted_caption)\n",
    "    return predicted_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_im, _ = train_data_generator.__getitem__(0)\n",
    "generate_caption(val_im)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
